{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are large regions of the planet which, although inhabited, remain unmapped to this day. In the past, DigitalGlobe has launched crowdsourcing campaigns to detect remote population centers in Ethiopia, Sudan and Swaziland in support of NGO vaccination and aid distribution initiatives. Beyond DigitalGlobe, there are other initiatives under way to fill in the gaps in the global map, aiding first responders in their effort to provide relief to vulnerable, yet inaccessible, people.\n",
    "\n",
    "The area of interest consists of 9 WorldView-2 and 2 GeoEye-1 image strips collected between January 2015 and May 2016 over northeastern Nigeria, close to as well as on the border with Niger and Cameroon. We picked 4 WorldView-2 strips, divided them in square chips of side 115m (250 pixels at sensor resolution) and asked our crowd to label them as ‘Buildings’ or ‘No Buildings’. The output of the crowdsourcing campaign is the file train.geojson which contains the labeled chip geometries.\n",
    "\n",
    "We used the labeled data to train the same neural net that we used [here](http://gbdxstories.digitalglobe.com/swimming-pools/) and deployed the trained model on the rest of the strips. \n",
    "\n",
    "The full story is [here](http://gbdxstories.digitalglobe.com/building-detection/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify your credentials and create a gbdx interface\n",
    "\n",
    "import os\n",
    "os.environ['GBDX_USERNAME'] = ''\n",
    "os.environ['GBDX_PASSWORD'] = ''\n",
    "os.environ['GBDX_CLIENT_ID'] = '' \n",
    "os.environ['GBDX_CLIENT_SECRET'] = ''\n",
    "\n",
    "import gbdxtools\n",
    "gbdx = gbdxtools.Interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import uuid\n",
    "\n",
    "# specify location of input files\n",
    "input_location = 's3://gbd-customer-data/32cbab7a-4307-40c8-bb31-e2de32f940c2/platform-stories/building-detection'\n",
    "\n",
    "# train task\n",
    "train_task = gbdx.Task('train-cnn-classifier')\n",
    "train_task.inputs.images = join(input_location, 'images')\n",
    "train_task.inputs.geojson = join(input_location, 'train-geojson')\n",
    "train_task.inputs.classes = 'No Buildings, Buildings'     # classes in train.geojson\n",
    "\n",
    "# set hyperparameters\n",
    "train_task.inputs.nb_epoch = '75'              # validation loss plateaus at around 60 - 70 epochs\n",
    "train_task.inputs.train_size = '5000'          # enough to get a reliable model, too much higher will make training too slow\n",
    "train_task.inputs.learning_rate = '0.001'\n",
    "train_task.inputs.max_side_dim = '245'         # chip side at sensor resolution\n",
    "train_task.inputs.resize_dim = '(3, 150, 150)' # down sample chips due to memory constraints\n",
    "train_task.inputs.two_rounds = 'False'         # second round results in low recall\n",
    "train_task.inputs.test_size = '1500'\n",
    "train_task.inputs.bit_depth = '8'      \n",
    "train_task.inputs.batch_size='32'              # low enough to fit into memory\n",
    "\n",
    "# deploy task\n",
    "deploy_tasks = {}\n",
    "deploy_ids = ['103001003D8CC700',       # WV-2\n",
    "              '1030010041B6F800',       # WV-2   \n",
    "              '1030010051A75500',       # WV-2  \n",
    "              '1030010054A8BD00',       # WV-2\n",
    "              '1030010055AF2D00',       # WV-2\n",
    "              '10504100120ADF00',       # GE-1\n",
    "              '1050410012CDC100']       # GE-1\n",
    "\n",
    "for catid in deploy_ids:\n",
    "    deploy_task = gbdx.Task('deploy-cnn-classifier')\n",
    "    deploy_task.inputs.model = train_task.outputs.trained_model.value     # Trained model from train_task\n",
    "    deploy_task.inputs.images = join(input_location, 'deploy-images', catid)\n",
    "    deploy_task.inputs.geojson = join(input_location, 'target-geojsons', catid)\n",
    "    deploy_task.inputs.classes = 'No Buildings, Buildings'\n",
    "    deploy_task.inputs.bit_depth = '8'\n",
    "    deploy_task.inputs.min_side_dim = '0'    \n",
    "    deploy_task.inputs.max_side_dim = '245'\n",
    "    deploy_tasks[catid] = deploy_task\n",
    "\n",
    "# define workflow\n",
    "workflow = gbdx.Workflow([train_task] + deploy_tasks.values())\n",
    "\n",
    "# set output location to platform-stories/trial-runs/random_str within your bucket/prefix\n",
    "random_str = str(uuid.uuid4())\n",
    "output_location = join('platform-stories/trial-runs', random_str)\n",
    "\n",
    "# save workflow outputs\n",
    "workflow.savedata(train_task.outputs.trained_model, join(output_location, 'trained_model'))\n",
    "\n",
    "# save output from each deploy_task\n",
    "for catid, task in deploy_tasks.iteritems():\n",
    "    workflow.savedata(task.outputs.classified_geojson, join(output_location, catid, 'classified_geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# execute workflow\n",
    "workflow.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workflow.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check under this location for the output\n",
    "print output_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create slippy map\n",
    "from ipyleaflet import Map, TileLayer\n",
    "\n",
    "m = Map(center=[13.48,12.69], zoom=12)\n",
    "\n",
    "# These are the IDAHO TMS urls for the pansharpened image and for the building heat map\n",
    "# We've saved them in IDAHO format in a custom bucket and are serving them through the TMS service\n",
    "url_image = 'http://idaho.geobigdata.io/v1/tile/platform-stories/969d2051-e456-4fa7-8d53-b5769287e069/{z}/{x}/{y}?bands=0,1,2&token=' + gbdx.gbdx_connection.access_token\n",
    "url_heatmap = 'http://idaho.geobigdata.io/v1/tile/platform-stories/f685a0a4-2425-4e3c-809e-c24c93813b31/{z}/{x}/{y}?bands=0,1,2&token=' + gbdx.gbdx_connection.access_token\n",
    "\n",
    "m.add_layer(TileLayer(url=url_image))\n",
    "m.add_layer(TileLayer(url=url_heatmap, opacity=0.5))\n",
    "    \n",
    "# launch map    \n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
